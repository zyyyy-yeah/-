import pandas as pd
import numpy as np
from datetime import datetime
import os
import glob
import random

def process_data_with_sampling(csv_folder_path, output_path, sample_frac=None, chunksize=50000):
    """
    æ”¯æŒæŠ½æ ·å’Œå…¨é‡å¤„ç†çš„æ•°æ®å¤„ç†å‡½æ•°
    """
    print("å¼€å§‹å¤„ç†æ•°æ®...")
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = glob.glob(os.path.join(csv_folder_path, "*.csv"))
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # åˆå§‹åŒ–è®¡æ•°å™¨
    total_processed = 0
    total_cleaned = 0
    chunk_count = 0
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶ï¼Œå…ˆå†™å…¥header
    first_chunk = True
    
    for file_idx, file in enumerate(csv_files):
        print(f"\nå¤„ç†æ–‡ä»¶ {file_idx+1}/{len(csv_files)}: {os.path.basename(file)}")
        
        # åˆ†å—è¯»å–æ–‡ä»¶
        for chunk_idx, chunk in enumerate(pd.read_csv(file, chunksize=chunksize, low_memory=False)):
            # å¦‚æœå¯ç”¨æŠ½æ ·ï¼Œåœ¨å—çº§åˆ«è¿›è¡ŒæŠ½æ ·
            if sample_frac is not None:
                # ç¡®ä¿æŠ½æ ·åçš„å—è‡³å°‘æœ‰ä¸€äº›æ•°æ®
                if len(chunk) > 10:  # åªæœ‰å—è¶³å¤Ÿå¤§æ—¶æ‰æŠ½æ ·
                    sample_size = max(1, int(len(chunk) * sample_frac))
                    chunk = chunk.sample(n=sample_size, random_state=42)
            
            chunk_count += 1
            total_processed += len(chunk)
            
            print(f"  å¤„ç†å— {chunk_idx+1}, å½“å‰å—å¤§å°: {len(chunk):,}")
            
            # å¤„ç†å½“å‰æ•°æ®å—
            cleaned_chunk = process_single_chunk(chunk)
            total_cleaned += len(cleaned_chunk)
            
            # å¦‚æœæ¸…æ´—åçš„å—ä¸ä¸ºç©ºï¼Œå†™å…¥æ–‡ä»¶
            if len(cleaned_chunk) > 0:
                if first_chunk:
                    cleaned_chunk.to_csv(output_path, index=False, mode='w')
                    first_chunk = False
                else:
                    cleaned_chunk.to_csv(output_path, index=False, mode='a', header=False)
            
            # æ¯å¤„ç†5ä¸ªå—è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if chunk_count % 5 == 0:
                print(f"    å·²å¤„ç† {chunk_count} ä¸ªå—ï¼Œæ€»è®°å½•: {total_processed:,}ï¼Œä¿ç•™: {total_cleaned:,}")
                
            # å¦‚æœå¯ç”¨äº†æŠ½æ ·ä¸”å·²ç»å¤„ç†äº†è¶³å¤Ÿçš„æ•°æ®ï¼Œå¯ä»¥æå‰é€€å‡º
            if sample_frac is not None and total_processed >= 100000:
                print(f"æŠ½æ ·æ•°æ®é‡å·²è¾¾åˆ° {total_processed:,}ï¼Œæå‰ç»“æŸå¤„ç†")
                break
                
        # æŠ½æ ·æ¨¡å¼ä¸‹ï¼Œå¦‚æœå·²ç»å¤„ç†äº†è¶³å¤Ÿæ•°æ®ï¼Œæå‰ç»“æŸæ–‡ä»¶å¾ªç¯
        if sample_frac is not None and total_processed >= 100000:
            break
    
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print(f"æ€»å¤„ç†è®°å½•: {total_processed:,}")
    print(f"æ¸…æ´—åè®°å½•: {total_cleaned:,}")
    print(f"æ•°æ®ä¿ç•™ç‡: {total_cleaned/total_processed*100:.2f}%")
    
    return total_processed, total_cleaned

def process_single_chunk(chunk):
    """
    å¤„ç†å•ä¸ªæ•°æ®å—
    """
    # 1. æ•°æ®ç±»å‹è½¬æ¢
    chunk['started_at'] = pd.to_datetime(chunk['started_at'], errors='coerce')
    chunk['ended_at'] = pd.to_datetime(chunk['ended_at'], errors='coerce')
    
    # 2. è®¡ç®—éª‘è¡Œæ—¶é•¿
    chunk['duration_minutes'] = (chunk['ended_at'] - chunk['started_at']).dt.total_seconds() / 60
    
    # 3. åˆ é™¤å…³é”®å­—æ®µç¼ºå¤±çš„è®°å½•
    critical_columns = ['started_at', 'ended_at', 'start_lat', 'start_lng', 'end_lat', 'end_lng']
    chunk_clean = chunk.dropna(subset=critical_columns)
    
    if len(chunk_clean) == 0:
        return pd.DataFrame()
    
    # 4. è¿‡æ»¤å¼‚å¸¸æ—¶é•¿ (1åˆ†é’Ÿåˆ°24å°æ—¶)
    chunk_clean = chunk_clean[
        (chunk_clean['duration_minutes'] >= 1) & 
        (chunk_clean['duration_minutes'] <= 24 * 60)
    ]
    
    if len(chunk_clean) == 0:
        return pd.DataFrame()
    
    # 5. è¿‡æ»¤å¼‚å¸¸åæ ‡
    chunk_clean = chunk_clean[
        (chunk_clean['start_lat'].between(-90, 90)) &
        (chunk_clean['start_lng'].between(-180, 180)) &
        (chunk_clean['end_lat'].between(-90, 90)) &
        (chunk_clean['end_lng'].between(-180, 180))
    ]
    
    if len(chunk_clean) == 0:
        return pd.DataFrame()
    
    # 6. æå–æ—¶é—´ç‰¹å¾
    chunk_clean['start_hour'] = chunk_clean['started_at'].dt.hour
    chunk_clean['start_dayofweek'] = chunk_clean['started_at'].dt.dayofweek
    chunk_clean['start_month'] = chunk_clean['started_at'].dt.month
    chunk_clean['start_date'] = chunk_clean['started_at'].dt.date
    chunk_clean['is_weekend'] = chunk_clean['start_dayofweek'].isin([5, 6])
    
    # 7. è®¡ç®—ç®€åŒ–è·ç¦»ï¼ˆé¿å…å¤æ‚è®¡ç®—èŠ‚çœå†…å­˜ï¼‰
    chunk_clean['distance_km'] = np.sqrt(
        (chunk_clean['end_lat'] - chunk_clean['start_lat'])**2 +
        (chunk_clean['end_lng'] - chunk_clean['start_lng'])**2
    ) * 111  # å¤§è‡´è½¬æ¢ä¸ºå…¬é‡Œ
    
    # 8. è¿‡æ»¤å¼‚å¸¸è·ç¦»
    chunk_clean = chunk_clean[chunk_clean['distance_km'].between(0.01, 50)]
    
    # 9. ä¼˜åŒ–æ•°æ®ç±»å‹å‡å°‘å†…å­˜
    categorical_columns = ['rideable_type', 'member_casual']
    for col in categorical_columns:
        if col in chunk_clean.columns and chunk_clean[col].notna().any():
            chunk_clean[col] = chunk_clean[col].astype('category')
    
    return chunk_clean

def analyze_final_data(output_path, sample_size=100000):
    """
    åˆ†ææœ€ç»ˆæ¸…æ´—åçš„æ•°æ®
    """
    print("\næ­£åœ¨åˆ†ææœ€ç»ˆæ•°æ®...")
    
    try:
        # è¯»å–æ•°æ®è¿›è¡Œåˆ†æ
        if os.path.getsize(output_path) > 100 * 1024 * 1024:  # å¦‚æœæ–‡ä»¶å¤§äº100MBï¼Œåªè¯»å–éƒ¨åˆ†
            df_sample = pd.read_csv(output_path, nrows=sample_size)
            print(f"æ–‡ä»¶è¾ƒå¤§ï¼Œä»…è¯»å–å‰ {sample_size:,} è¡Œè¿›è¡Œåˆ†æ")
        else:
            df_sample = pd.read_csv(output_path)
        
        print("\n=== æ•°æ®æ¦‚è§ˆ ===")
        print(f"æ•°æ®å½¢çŠ¶: {df_sample.shape}")
        print(f"åˆ—å: {list(df_sample.columns)}")
        
        print(f"\nç”¨æˆ·ç±»å‹åˆ†å¸ƒ:")
        print(df_sample['member_casual'].value_counts())
        
        print(f"\nè½¦è¾†ç±»å‹åˆ†å¸ƒ:")
        print(df_sample['rideable_type'].value_counts())
        
        if 'duration_minutes' in df_sample.columns:
            print(f"\néª‘è¡Œæ—¶é•¿ç»Ÿè®¡:")
            print(f"  å¹³å‡: {df_sample['duration_minutes'].mean():.2f} åˆ†é’Ÿ")
            print(f"  ä¸­ä½æ•°: {df_sample['duration_minutes'].median():.2f} åˆ†é’Ÿ")
            print(f"  æœ€å¤§: {df_sample['duration_minutes'].max():.2f} åˆ†é’Ÿ")
            print(f"  æœ€å°: {df_sample['duration_minutes'].min():.2f} åˆ†é’Ÿ")
        
        if 'distance_km' in df_sample.columns:
            print(f"\néª‘è¡Œè·ç¦»ç»Ÿè®¡:")
            print(f"  å¹³å‡: {df_sample['distance_km'].mean():.2f} å…¬é‡Œ")
            print(f"  ä¸­ä½æ•°: {df_sample['distance_km'].median():.2f} å…¬é‡Œ")
        
        print(f"\næ—¶é—´èŒƒå›´:")
        if 'started_at' in df_sample.columns:
            # è½¬æ¢å›datetimeç”¨äºåˆ†æ
            df_sample['started_at'] = pd.to_datetime(df_sample['started_at'])
            print(f"  å¼€å§‹: {df_sample['started_at'].min()}")
            print(f"  ç»“æŸ: {df_sample['started_at'].max()}")
            
    except Exception as e:
        print(f"åˆ†ææ•°æ®æ—¶å‡ºé”™: {e}")

def main():
    """
    ä¸»å‡½æ•°
    """
    # é…ç½®å‚æ•° - åœ¨è¿™é‡Œåˆ‡æ¢æ¨¡å¼ï¼
    CSV_FOLDER_PATH = "E:/10-å¤§ä¸‰ä¸Š/1-å­¦ä¹ /1-ä¿¡ç®¡/3-å¤§æ•°æ®ç³»ç»ŸåŸç†ä¸åº”ç”¨/æœŸä¸­ä½œä¸š/202506-citibike-tripdata"  # ä¿®æ”¹ä¸ºæ‚¨çš„CSVæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„
    OUTPUT_PATH = "E:/10-å¤§ä¸‰ä¸Š/1-å­¦ä¹ /1-ä¿¡ç®¡/3-å¤§æ•°æ®ç³»ç»ŸåŸç†ä¸åº”ç”¨/æœŸä¸­ä½œä¸š/bike.csv"
    CHUNKSIZE = 50000  # æ¯ä¸ªå—çš„å¤§å°
    
    # === é€‰æ‹©è¿è¡Œæ¨¡å¼ ===
    # MODE 1: æŠ½æ ·æµ‹è¯• (æ¨èå…ˆè¿è¡Œè¿™ä¸ª)
    SAMPLE_FRAC = None  # 1% çš„æŠ½æ ·ç‡
    
    # MODE 2: å…¨é‡å¤„ç† (ç¡®è®¤æµ‹è¯•æ— è¯¯åä½¿ç”¨)
    # SAMPLE_FRAC = None  # å¤„ç†å…¨éƒ¨æ•°æ®
    
    print("=== å…±äº«å•è½¦æ•°æ®é¢„å¤„ç† (æŠ½æ ·/å…¨é‡å¯é€‰ç‰ˆ) ===")
    print(f"è¾“å…¥è·¯å¾„: {CSV_FOLDER_PATH}")
    print(f"è¾“å‡ºè·¯å¾„: {OUTPUT_PATH}")
    print(f"å—å¤§å°: {CHUNKSIZE:,}")
    
    if SAMPLE_FRAC is not None:
        print(f"è¿è¡Œæ¨¡å¼: æŠ½æ ·æ¨¡å¼ ({SAMPLE_FRAC*100}% æ•°æ®)")
    else:
        print("è¿è¡Œæ¨¡å¼: å…¨é‡æ¨¡å¼")
    
    try:
        # å¤„ç†æ•°æ®
        total_processed, total_cleaned = process_data_with_sampling(
            CSV_FOLDER_PATH, OUTPUT_PATH, SAMPLE_FRAC, CHUNKSIZE
        )
        
        # åˆ†æç»“æœ
        analyze_final_data(OUTPUT_PATH)
        
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}")
        file_size = os.path.getsize(OUTPUT_PATH) / (1024*1024)
        print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # ä½¿ç”¨å»ºè®®
        if SAMPLE_FRAC is not None:
            print(f"\nğŸ’¡ æç¤º: æŠ½æ ·æµ‹è¯•æˆåŠŸï¼ç°åœ¨æ‚¨å¯ä»¥ä¿®æ”¹ SAMPLE_FRAC = None æ¥è¿è¡Œå…¨é‡æ•°æ®")
        else:
            print(f"\nğŸ‰ å…¨é‡æ•°æ®å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()