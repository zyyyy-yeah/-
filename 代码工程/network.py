import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import zipfile
import os
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è‹±æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']
plt.rcParams['axes.unicode_minus'] = False

# ä½¿ç”¨æ‚¨æä¾›çš„ä½é¥±å’Œåº¦é…è‰²æ–¹æ¡ˆ
colors = {
    'normal': '#7895C1',
    'extreme': '#E3625D', 
    'positive': '#992224',
    'negative': '#8074C8',
    'decrease': '#E3625D',
    'increase': '#7895C1',
    'accent1': '#F0C284',
    'accent2': '#F5EBAE',
    'light_blue': '#A8CBDF',
    'very_light_blue': '#D6EFF4',
    'lightest_blue': '#F2FAFC'
}

# è¯·åœ¨è¿™é‡Œæä¾›æ‚¨çš„æ•°æ®æ–‡ä»¶è·¯å¾„
BIKE_PATH = r"D:\ä½œä¸š\å¤§ä¸‰ä¸Šè¯¾ç¨‹\å¤§æ•°æ®åŸç†ä¸åº”ç”¨\æœŸåˆä½œä¸š\bike.csv"
WEATHER_PATH = r"D:\ä½œä¸š\å¤§ä¸‰ä¸Šè¯¾ç¨‹\å¤§æ•°æ®åŸç†ä¸åº”ç”¨\æœŸåˆä½œä¸š\daily-summaries-2025-10-09T12-21-41.xlsx"
NYWIND_PATH = r"D:\ä½œä¸š\å¤§ä¸‰ä¸Šè¯¾ç¨‹\å¤§æ•°æ®åŸç†ä¸åº”ç”¨\æœŸåˆä½œä¸š\nywind.xlsx"
SUBWAY_PATH = r"D:\ä½œä¸š\å¤§ä¸‰ä¸Šè¯¾ç¨‹\å¤§æ•°æ®åŸç†ä¸åº”ç”¨\æœŸåˆä½œä¸š\gtfs_subway.zip"
BUS_PATH = r"D:\ä½œä¸š\å¤§ä¸‰ä¸Šè¯¾ç¨‹\å¤§æ•°æ®åŸç†ä¸åº”ç”¨\æœŸåˆä½œä¸š\gtfs_bus.zip"

def load_gtfs_data(gtfs_path, data_type):
    """
    åŠ è½½GTFSæ•°æ®
    """
    try:
        with zipfile.ZipFile(gtfs_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            print(f"  {data_type} files: {file_list}")
            
            # è¯»å–ç«™ç‚¹æ•°æ®
            if 'stops.txt' in file_list:
                with zip_ref.open('stops.txt') as f:
                    stops_df = pd.read_csv(f)
                print(f"  Loaded {len(stops_df)} stations")
                
                # é‡å‘½ååˆ—
                stops_df = stops_df.rename(columns={
                    'stop_id': 'station_id',
                    'stop_name': 'station_name',
                    'stop_lat': 'latitude', 
                    'stop_lon': 'longitude'
                })
                
                return stops_df[['station_id', 'station_name', 'latitude', 'longitude']]
            else:
                print(f"  stops.txt not found")
                return pd.DataFrame()
                
    except Exception as e:
        print(f"Error loading {data_type} data: {e}")
        return pd.DataFrame()

def load_bus_data(bus_path):
    """
    åŠ è½½å…¬äº¤æ•°æ®
    """
    bus_regions = {}
    
    try:
        with zipfile.ZipFile(bus_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            print(f"  Bus files: {file_list}")
            
            # ä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶åæ˜ å°„
            region_files = {
                'Bronx': 'gtf5_bx.zip',
                'Brooklyn': 'gtf5_b.zip', 
                'Manhattan': 'gtf5_m.zip',
                'Queens': 'gtf5_q.zip',
                'Staten Island': 'gtf5_si.zip',
                'Bus Company': 'gtf5_busco.zip'
            }
            
            loaded_regions = 0
            for region_name, file_name in region_files.items():
                if file_name in file_list:
                    print(f"  Loading {region_name} bus data: {file_name}")
                    
                    with zip_ref.open(file_name) as region_file:
                        # ä¸´æ—¶ä¿å­˜åŒºåŸŸæ–‡ä»¶
                        temp_path = f"temp_{region_name}.zip"
                        with open(temp_path, 'wb') as f:
                            f.write(region_file.read())
                        
                        # è§£æåŒºåŸŸGTFS
                        region_data = load_gtfs_data(temp_path, f"{region_name} bus")
                        if not region_data.empty:
                            region_data['region'] = region_name
                            bus_regions[region_name] = region_data
                            loaded_regions += 1
                            print(f"    âœ“ Successfully loaded {region_name} region, {len(region_data)} stations")
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if os.path.exists(temp_path):
                            os.remove(temp_path)
                else:
                    print(f"  âœ— {region_name} region file not found: {file_name}")
        
        print(f"  Successfully loaded {loaded_regions} bus regions")
        return bus_regions
        
    except Exception as e:
        print(f"Error loading bus data: {e}")
        return {}

def load_bike_data_simple(file_path, sample_fraction=0.01):
    """
    ç®€åŒ–ç‰ˆå•è½¦æ•°æ®åŠ è½½
    """
    print(f"Loading bike data: {file_path}")
    
    try:
        # åªè¯»å–å‰å‡ è¡Œäº†è§£æ•°æ®ç»“æ„
        sample_rows = pd.read_csv(file_path, nrows=5)
        print(f"Bike data columns: {list(sample_rows.columns)}")
        
        # å¦‚æœæ–‡ä»¶å¾ˆå¤§ï¼Œåªé‡‡æ ·ä¸€å°éƒ¨åˆ†
        file_size = os.path.getsize(file_path) / (1024**3)  # GB
        if file_size > 0.1:  # å¦‚æœæ–‡ä»¶å¤§äº100MB
            print(f"File is large ({file_size:.2f} GB), sampling {sample_fraction*100}%")
            bike_data = pd.read_csv(file_path, nrows=int(100000 * sample_fraction))
        else:
            bike_data = pd.read_csv(file_path)
        
        print(f"Loaded bike data with {len(bike_data)} rows")
        return bike_data
        
    except Exception as e:
        print(f"Error loading bike data: {e}")
        # åˆ›å»ºç¤ºä¾‹æ•°æ®ç»§ç»­åˆ†æ
        print("Creating sample bike data...")
        return create_sample_bike_data()

def create_sample_bike_data():
    """åˆ›å»ºç¤ºä¾‹å•è½¦æ•°æ®"""
    np.random.seed(42)
    n_records = 10000
    
    bike_data = pd.DataFrame({
        'ride_id': [f'ride_{i}' for i in range(n_records)],
        'start_station_name': np.random.choice([
            'Mercer St & Bleecker St', '1 St & Bowery', 'Broadway & W 58 St',
            '8 Ave & W 31 St', 'E 23 St & 1 Ave'
        ], n_records),
        'end_station_name': np.random.choice([
            'W 41 St & 8 Ave', 'E 17 St & Broadway', 'W 33 St & 7 Ave',
            'Forsyth St & Broome St', 'Allen St & Rivington St'
        ], n_records),
    })
    
    return bike_data

def load_all_data():
    """
    åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®
    """
    print("Loading all data...")
    
    data_dict = {}
    
    # 1. åŠ è½½å•è½¦æ•°æ®
    data_dict['bike'] = load_bike_data_simple(BIKE_PATH)
    
    # 2. åŠ è½½åœ°é“æ•°æ®
    print(f"Loading subway data: {SUBWAY_PATH}")
    data_dict['subway'] = load_gtfs_data(SUBWAY_PATH, 'subway')
    
    # 3. åŠ è½½å…¬äº¤æ•°æ®
    print(f"Loading bus data: {BUS_PATH}")
    data_dict['bus'] = load_bus_data(BUS_PATH)
    
    # 4. åŠ è½½å¤©æ°”æ•°æ®ï¼ˆå¯é€‰ï¼‰
    try:
        print(f"Loading weather data: {WEATHER_PATH}")
        data_dict['weather'] = pd.read_excel(WEATHER_PATH)
        print(f"Weather data shape: {data_dict['weather'].shape}")
    except:
        print("Weather data not available, continuing without it")
        data_dict['weather'] = pd.DataFrame()
    
    return data_dict

def create_simplified_network(data_dict):
    """
    åˆ›å»ºç®€åŒ–çš„äº¤é€šç½‘ç»œ
    """
    print("Creating simplified transportation network...")
    
    subway_data = data_dict['subway']
    bus_data = data_dict['bus']
    
    # åˆ›å»ºç½‘ç»œå›¾
    G = nx.Graph()
    
    # æ·»åŠ åœ°é“ç«™ç‚¹
    if not subway_data.empty:
        # å¦‚æœåœ°é“ç«™ç‚¹å¤ªå¤šï¼Œé‡‡æ ·ä¸€éƒ¨åˆ†
        if len(subway_data) > 500:
            subway_data = subway_data.sample(n=500, random_state=42)
            print(f"  Sampling 500 subway stations from {len(subway_data)} total")
        
        for idx, station in subway_data.iterrows():
            G.add_node(f"subway_{station['station_id']}", 
                      node_type='subway',
                      name=station['station_name'],
                      lat=station['latitude'],
                      lon=station['longitude'])
        print(f"  Added {len(subway_data)} subway stations")
    
    # æ·»åŠ å…¬äº¤ç«™ç‚¹ï¼ˆä»æ‰€æœ‰åŒºåŸŸï¼‰
    bus_station_count = 0
    if bus_data:
        for region_name, region_data in bus_data.items():
            # æ¯ä¸ªåŒºåŸŸåªå–å‰100ä¸ªç«™ç‚¹é¿å…ç½‘ç»œå¤ªå¤§
            region_sample = region_data.head(100)
            for idx, stop in region_sample.iterrows():
                G.add_node(f"bus_{region_name}_{stop['station_id']}",
                          node_type='bus',
                          name=stop['station_name'],
                          lat=stop['latitude'],
                          lon=stop['longitude'],
                          region=region_name)
                bus_station_count += 1
        print(f"  Added {bus_station_count} bus stations")
    
    print(f"Network created with {len(G.nodes())} total nodes")
    
    # æ·»åŠ è¿æ¥ï¼ˆåŸºäºè·ç¦»ï¼‰
    print("  Adding connections based on spatial proximity...")
    nodes_list = list(G.nodes())
    added_edges = 0
    
    # åªæ·»åŠ æœ€è¿‘çš„å‡ ä¸ªé‚»å±…ï¼Œé¿å…è¿‡å¤šè¿æ¥
    for i, node_i in enumerate(nodes_list):
        if i % 100 == 0 and i > 0:
            print(f"    Processed {i}/{len(nodes_list)} nodes, added {added_edges} edges")
        
        lat_i = G.nodes[node_i]['lat']
        lon_i = G.nodes[node_i]['lon']
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ 3-5ä¸ªæœ€è¿‘é‚»è¿æ¥
        distances = []
        for j, node_j in enumerate(nodes_list):
            if i != j:
                lat_j = G.nodes[node_j]['lat']
                lon_j = G.nodes[node_j]['lon']
                # è®¡ç®—è¿‘ä¼¼è·ç¦»
                distance = np.sqrt((lat_i - lat_j)**2 + (lon_i - lon_j)**2)
                distances.append((node_j, distance))
        
        # æ·»åŠ æœ€è¿‘çš„å‡ ä¸ªè¿æ¥
        distances.sort(key=lambda x: x[1])
        for k in range(min(3, len(distances))):
            if distances[k][1] < 0.02:  # åªæ·»åŠ ç›¸å¯¹è¾ƒè¿‘çš„è¿æ¥
                G.add_edge(node_i, distances[k][0], weight=1.0/distances[k][1])
                added_edges += 1
    
    print(f"Network complete with {len(G.nodes())} nodes and {len(G.edges())} edges")
    return G

def calculate_basic_centrality(G):
    """
    è®¡ç®—åŸºç¡€çš„ä¸­å¿ƒæ€§æŒ‡æ ‡
    """
    print("Calculating basic centrality measures...")
    
    centrality_results = {}
    
    # 1. åº¦ä¸­å¿ƒæ€§
    print("  - Degree centrality...")
    centrality_results['degree'] = nx.degree_centrality(G)
    
    # 2. ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆä½¿ç”¨è¿‘ä¼¼ç®—æ³•ï¼‰
    print("  - Betweenness centrality (approximate)...")
    try:
        # ä½¿ç”¨æ›´å°çš„æ ·æœ¬æ•°
        k = min(50, len(G.nodes()) // 10)
        centrality_results['betweenness'] = nx.betweenness_centrality(G, k=k)
    except:
        print("    Betweenness calculation failed, using degree as fallback")
        centrality_results['betweenness'] = centrality_results['degree']
    
    return centrality_results

def create_simple_visualizations(G, centrality_results):
    """
    åˆ›å»ºç®€åŒ–çš„å¯è§†åŒ–
    """
    print("Creating simplified visualizations...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Transportation Network Centrality Analysis', fontsize=16, fontweight='bold')
    
    # 1. åº¦ä¸­å¿ƒæ€§åˆ†å¸ƒ
    degree_values = list(centrality_results['degree'].values())
    ax1.hist(degree_values, bins=20, 
             alpha=0.7, color=colors['normal'], edgecolor='white')
    ax1.set_xlabel('Degree Centrality', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.set_title('A) Degree Centrality Distribution', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    mean_deg = np.mean(degree_values)
    ax1.axvline(mean_deg, color=colors['extreme'], linestyle='--', 
                label=f'Mean: {mean_deg:.4f}')
    ax1.legend()
    
    # 2. ä»‹æ•°ä¸­å¿ƒæ€§åˆ†å¸ƒ
    betweenness_values = list(centrality_results['betweenness'].values())
    ax2.hist(betweenness_values, bins=20,
             alpha=0.7, color=colors['accent1'], edgecolor='white')
    ax2.set_xlabel('Betweenness Centrality', fontsize=12)
    ax2.set_ylabel('Frequency', fontsize=12)
    ax2.set_title('B) Betweenness Centrality Distribution', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    mean_bet = np.mean(betweenness_values)
    ax2.axvline(mean_bet, color=colors['extreme'], linestyle='--', 
                label=f'Mean: {mean_bet:.4f}')
    ax2.legend()
    
    # 3. å‰10ä¸ªå…³é”®èŠ‚ç‚¹ï¼ˆæŒ‰åº¦ä¸­å¿ƒæ€§ï¼‰
    top_degree = sorted(centrality_results['degree'].items(), 
                       key=lambda x: x[1], reverse=True)[:10]
    
    node_names = []
    degree_scores = []
    for node, score in top_degree:
        name = G.nodes[node].get('name', f'Node {node}')
        # ç¼©çŸ­é•¿åç§°
        if len(name) > 25:
            name = name[:22] + '...'
        node_names.append(name)
        degree_scores.append(score)
    
    y_pos = np.arange(len(node_names))
    bars = ax3.barh(y_pos, degree_scores, color=colors['normal'], alpha=0.7)
    ax3.set_yticks(y_pos)
    ax3.set_yticklabels(node_names, fontsize=9)
    ax3.set_xlabel('Degree Centrality Score', fontsize=12)
    ax3.set_title('C) Top 10 Nodes by Degree Centrality', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    for i, (bar, score) in enumerate(zip(bars, degree_scores)):
        ax3.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                f'{score:.4f}', ha='left', va='center', fontsize=9)
    
    # 4. å‰10ä¸ªå…³é”®èŠ‚ç‚¹ï¼ˆæŒ‰ä»‹æ•°ä¸­å¿ƒæ€§ï¼‰
    top_betweenness = sorted(centrality_results['betweenness'].items(), 
                           key=lambda x: x[1], reverse=True)[:10]
    
    node_names_bt = []
    betweenness_scores = []
    for node, score in top_betweenness:
        name = G.nodes[node].get('name', f'Node {node}')
        if len(name) > 25:
            name = name[:22] + '...'
        node_names_bt.append(name)
        betweenness_scores.append(score)
    
    y_pos_bt = np.arange(len(node_names_bt))
    bars_bt = ax4.barh(y_pos_bt, betweenness_scores, color=colors['extreme'], alpha=0.7)
    ax4.set_yticks(y_pos_bt)
    ax4.set_yticklabels(node_names_bt, fontsize=9)
    ax4.set_xlabel('Betweenness Centrality Score', fontsize=12)
    ax4.set_title('D) Top 10 Nodes by Betweenness Centrality', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    for i, (bar, score) in enumerate(zip(bars_bt, betweenness_scores)):
        ax4.text(bar.get_width() + 0.0001, bar.get_y() + bar.get_height()/2,
                f'{score:.4f}', ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('network_centrality_analysis.png', dpi=300, bbox_inches='tight',
                facecolor=colors['lightest_blue'])
    plt.show()
    
    return fig

def generate_analysis_report(G, centrality_results):
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    """
    print("\n" + "="*80)
    print("NETWORK CENTRALITY ANALYSIS REPORT")
    print("="*80)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š Network Statistics:")
    print(f"   â€¢ Total nodes: {len(G.nodes())}")
    print(f"   â€¢ Total edges: {len(G.edges())}")
    print(f"   â€¢ Network density: {nx.density(G):.4f}")
    
    # èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
    node_types = {}
    for node in G.nodes():
        node_type = G.nodes[node].get('node_type', 'unknown')
        node_types[node_type] = node_types.get(node_type, 0) + 1
    
    print(f"   â€¢ Node type distribution:")
    for node_type, count in node_types.items():
        print(f"     - {node_type}: {count} nodes ({count/len(G.nodes())*100:.1f}%)")
    
    # ä¸­å¿ƒæ€§ç»Ÿè®¡
    print(f"\nğŸ¯ Centrality Analysis:")
    
    degree_values = list(centrality_results['degree'].values())
    betweenness_values = list(centrality_results['betweenness'].values())
    
    print(f"   â€¢ Degree Centrality:")
    print(f"     - Mean: {np.mean(degree_values):.4f}")
    print(f"     - Std:  {np.std(degree_values):.4f}")
    print(f"     - Max:  {np.max(degree_values):.4f}")
    
    print(f"   â€¢ Betweenness Centrality:")
    print(f"     - Mean: {np.mean(betweenness_values):.4f}")
    print(f"     - Std:  {np.std(betweenness_values):.4f}")
    print(f"     - Max:  {np.max(betweenness_values):.4f}")
    
    # å…³é”®èŠ‚ç‚¹è¯†åˆ«
    print(f"\nğŸ” Critical Node Identification:")
    
    print(f"   â€¢ Top 5 Nodes by Degree Centrality (Most Connected):")
    top_degree = sorted(centrality_results['degree'].items(), 
                       key=lambda x: x[1], reverse=True)[:5]
    for i, (node, score) in enumerate(top_degree, 1):
        name = G.nodes[node].get('name', 'Unknown Station')
        node_type = G.nodes[node].get('node_type', 'unknown')
        print(f"     {i}. {name} ({node_type}) - Score: {score:.4f}")
    
    print(f"   â€¢ Top 5 Nodes by Betweenness Centrality (Network Bridges):")
    top_betweenness = sorted(centrality_results['betweenness'].items(), 
                           key=lambda x: x[1], reverse=True)[:5]
    for i, (node, score) in enumerate(top_betweenness, 1):
        name = G.nodes[node].get('name', 'Unknown Station')
        node_type = G.nodes[node].get('node_type', 'unknown')
        print(f"     {i}. {name} ({node_type}) - Score: {score:.4f}")
    
    # è„†å¼±æ€§åˆ†æ
    print(f"\nâš ï¸  Vulnerability Assessment:")
    
    # è¯†åˆ«é«˜ä»‹æ•°èŠ‚ç‚¹ï¼ˆç½‘ç»œç“¶é¢ˆï¼‰
    bottleneck_nodes = top_betweenness[:3]
    print(f"   â€¢ Critical Bottlenecks (High Betweenness):")
    for i, (node, score) in enumerate(bottleneck_nodes, 1):
        name = G.nodes[node].get('name', 'Unknown Station')
        print(f"     {i}. {name}")
        print(f"        - Acts as critical bridge in the network")
        print(f"        - Failure would significantly disrupt connectivity")
    
    # è¯†åˆ«é«˜åº¦è¿æ¥èŠ‚ç‚¹
    hub_nodes = top_degree[:3]
    print(f"   â€¢ Major Hubs (High Degree):")
    for i, (node, score) in enumerate(hub_nodes, 1):
        name = G.nodes[node].get('name', 'Unknown Station')
        print(f"     {i}. {name}")
        print(f"        - Central station with many connections")
        print(f"        - Important for local connectivity")
    
    # æ”¹è¿›å»ºè®®
    print(f"\nğŸ¯ Resilience Improvement Recommendations:")
    print(f"   1. Reinforce {bottleneck_nodes[0][0].split('_')[-1]} with backup systems")
    print(f"   2. Develop contingency plans for {hub_nodes[0][0].split('_')[-1]}")
    print(f"   3. Improve alternative routes around critical nodes")
    print(f"   4. Monitor these stations during extreme weather events")
    print(f"   5. Consider adding redundant connections to bottleneck nodes")

def perform_network_analysis():
    """
    æ‰§è¡Œå®Œæ•´çš„ç½‘ç»œåˆ†æ
    """
    print("Starting Network Centrality Analysis")
    print("="*50)
    
    try:
        # 1. åŠ è½½æ•°æ®
        data_dict = load_all_data()
        
        # 2. åˆ›å»ºç½‘ç»œ
        G = create_simplified_network(data_dict)
        
        if len(G.nodes()) == 0:
            print("âŒ No nodes in network. Check your data.")
            return None
        
        # 3. è®¡ç®—ä¸­å¿ƒæ€§
        centrality_results = calculate_basic_centrality(G)
        
        # 4. åˆ›å»ºå¯è§†åŒ–
        fig = create_simple_visualizations(G, centrality_results)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        generate_analysis_report(G, centrality_results)
        
        print(f"\nâœ… Network analysis completed successfully!")
        print(f"ğŸ“Š Generated: network_centrality_analysis.png")
        
        return {
            'network': G,
            'centrality': centrality_results
        }
        
    except Exception as e:
        print(f"âŒ Error in network analysis: {e}")
        import traceback
        traceback.print_exc()
        return None

# ä¸»æ‰§è¡Œå‡½æ•°
if __name__ == "__main__":
    print("ğŸš€ Starting Network Centrality Analysis")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    print("Checking data files...")
    essential_files = [SUBWAY_PATH, BUS_PATH]
    for file_path in essential_files:
        if os.path.exists(file_path):
            print(f"âœ… {os.path.basename(file_path)} - Found")
        else:
            print(f"âŒ {os.path.basename(file_path)} - Not found")
    
    print("\n" + "="*50)
    
    # æ‰§è¡Œåˆ†æ
    results = perform_network_analysis()
    
    if results:
        print("\n" + "="*60)
        print("Analysis completed successfully!")
        print("Check the generated file: network_centrality_analysis.png")
        print("="*60)
    else:
        print("âŒ Analysis failed. Please check the error messages above.")